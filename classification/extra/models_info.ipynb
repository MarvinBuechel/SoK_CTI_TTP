{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model: bert-base-uncased ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model: bert-base-cased ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model: FacebookAI/roberta-base ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at FacebookAI/roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model: FacebookAI/xlm-roberta-base ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at FacebookAI/xlm-roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model: FacebookAI/roberta-large ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at FacebookAI/roberta-large and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model: FacebookAI/xlm-roberta-large ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at FacebookAI/xlm-roberta-large and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model: s2w-ai/DarkBERT ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at s2w-ai/DarkBERT and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model: jackaduma/SecBERT ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at jackaduma/SecBERT and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model: jackaduma/SecRoBERTa ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at jackaduma/SecRoBERTa and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model: markusbayer/CySecBERT ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at markusbayer/CySecBERT and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model: allenai/scibert_scivocab_cased ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at allenai/scibert_scivocab_cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model: allenai/scibert_scivocab_uncased ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at allenai/scibert_scivocab_uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model: priyankaranade/cybert ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at local/CyBERT-Base-MLM-v1.1 and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model: tram_multi_label_model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at local/tram_finetuned and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model: ehsanaghaei/SecureBERT ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at ehsanaghaei/SecureBERT and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model: sentence-transformers/all-mpnet-base-v2 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of MPNetForSequenceClassification were not initialized from the model checkpoint at sentence-transformers/all-mpnet-base-v2 and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model: basel/ATTACK-BERT ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of MPNetForSequenceClassification were not initialized from the model checkpoint at basel/ATTACK-BERT and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model: qcri-cs/SentSecBert_10k ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at qcri-cs/SentSecBert_10k and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModel\n",
    "import loader\n",
    "from decimal import Decimal\n",
    "\n",
    "MODELS = [\n",
    "    \"bert-base-uncased\",\n",
    "    \"bert-base-cased\",\n",
    "    \"FacebookAI/roberta-base\",\n",
    "    \"FacebookAI/xlm-roberta-base\",\n",
    "    \"FacebookAI/roberta-large\",\n",
    "    \"FacebookAI/xlm-roberta-large\",\n",
    "    \"s2w-ai/DarkBERT\",\n",
    "    \"jackaduma/SecBERT\",\n",
    "    \"jackaduma/SecRoBERTa\",\n",
    "    \"markusbayer/CySecBERT\",\n",
    "    \"allenai/scibert_scivocab_cased\",\n",
    "    \"allenai/scibert_scivocab_uncased\",\n",
    "    \"priyankaranade/cybert\",\n",
    "    \"tram_multi_label_model\",\n",
    "    \"ehsanaghaei/SecureBERT\",\n",
    "    \"sentence-transformers/all-mpnet-base-v2\", \n",
    "    \"basel/ATTACK-BERT\",\n",
    "    \"qcri-cs/SentSecBert_10k\",\n",
    "]\n",
    "\n",
    "\n",
    "\n",
    "table = {\n",
    "    \"models\": [],\n",
    "    \"parameters\": []\n",
    "}\n",
    "\n",
    "for model_name in MODELS:\n",
    "    # Load the model in 8-bit quantization using the bitsandbytes library\n",
    "    model, _ = loader.load_untrained_model(model_name, \"tram\")\n",
    "    # Count the number of parameters\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    total_params = '%.2E' % Decimal(total_params)\n",
    "    table[\"models\"].append(model_name)\n",
    "    table[\"parameters\"].append(total_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>models</th>\n",
       "      <th>parameters</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>bert-base-uncased</td>\n",
       "      <td>1.10E+08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>bert-base-cased</td>\n",
       "      <td>1.08E+08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>FacebookAI/roberta-base</td>\n",
       "      <td>1.25E+08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>FacebookAI/xlm-roberta-base</td>\n",
       "      <td>2.78E+08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>FacebookAI/roberta-large</td>\n",
       "      <td>3.55E+08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>FacebookAI/xlm-roberta-large</td>\n",
       "      <td>5.60E+08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>s2w-ai/DarkBERT</td>\n",
       "      <td>1.25E+08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>jackaduma/SecBERT</td>\n",
       "      <td>8.35E+07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>jackaduma/SecRoBERTa</td>\n",
       "      <td>8.35E+07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>markusbayer/CySecBERT</td>\n",
       "      <td>1.10E+08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>allenai/scibert_scivocab_cased</td>\n",
       "      <td>1.10E+08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>allenai/scibert_scivocab_uncased</td>\n",
       "      <td>1.10E+08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>priyankaranade/cybert</td>\n",
       "      <td>1.11E+08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>tram_multi_label_model</td>\n",
       "      <td>1.10E+08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>ehsanaghaei/SecureBERT</td>\n",
       "      <td>1.25E+08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>sentence-transformers/all-mpnet-base-v2</td>\n",
       "      <td>1.10E+08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>basel/ATTACK-BERT</td>\n",
       "      <td>1.10E+08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>qcri-cs/SentSecBert_10k</td>\n",
       "      <td>8.35E+07</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     models parameters\n",
       "0                         bert-base-uncased   1.10E+08\n",
       "1                           bert-base-cased   1.08E+08\n",
       "2                   FacebookAI/roberta-base   1.25E+08\n",
       "3               FacebookAI/xlm-roberta-base   2.78E+08\n",
       "4                  FacebookAI/roberta-large   3.55E+08\n",
       "5              FacebookAI/xlm-roberta-large   5.60E+08\n",
       "6                           s2w-ai/DarkBERT   1.25E+08\n",
       "7                         jackaduma/SecBERT   8.35E+07\n",
       "8                      jackaduma/SecRoBERTa   8.35E+07\n",
       "9                     markusbayer/CySecBERT   1.10E+08\n",
       "10           allenai/scibert_scivocab_cased   1.10E+08\n",
       "11         allenai/scibert_scivocab_uncased   1.10E+08\n",
       "12                    priyankaranade/cybert   1.11E+08\n",
       "13                   tram_multi_label_model   1.10E+08\n",
       "14                   ehsanaghaei/SecureBERT   1.25E+08\n",
       "15  sentence-transformers/all-mpnet-base-v2   1.10E+08\n",
       "16                        basel/ATTACK-BERT   1.10E+08\n",
       "17                  qcri-cs/SentSecBert_10k   8.35E+07"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "pd.DataFrame(table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:02<00:00,  1.20it/s]\n",
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  6.35it/s]\n",
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:03<00:00,  1.13it/s]\n"
     ]
    }
   ],
   "source": [
    "models = [\n",
    "    # \"meta-llama/Meta-Llama-3-8B\",\n",
    "    'Salesforce/SFR-Embedding-2_R',\n",
    "    \"BAAI/bge-en-icl\",\n",
    "    \"nvidia/NV-Embed-V2\"\n",
    "]\n",
    "\n",
    "\n",
    "for model_name in models:\n",
    "    # Load the model in 8-bit quantization using the bitsandbytes library\n",
    "    model = AutoModel.from_pretrained(model_name)\n",
    "    # Count the number of parameters\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    total_params = '%.2E' % Decimal(total_params)\n",
    "\n",
    "    table[\"models\"].append(model_name)\n",
    "    table[\"parameters\"].append(total_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>models</th>\n",
       "      <th>parameters</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>bert-base-uncased</td>\n",
       "      <td>1.10E+08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>bert-base-cased</td>\n",
       "      <td>1.08E+08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>FacebookAI/roberta-base</td>\n",
       "      <td>1.25E+08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>FacebookAI/xlm-roberta-base</td>\n",
       "      <td>2.78E+08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>FacebookAI/roberta-large</td>\n",
       "      <td>3.55E+08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>FacebookAI/xlm-roberta-large</td>\n",
       "      <td>5.60E+08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>s2w-ai/DarkBERT</td>\n",
       "      <td>1.25E+08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>jackaduma/SecBERT</td>\n",
       "      <td>8.35E+07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>jackaduma/SecRoBERTa</td>\n",
       "      <td>8.35E+07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>markusbayer/CySecBERT</td>\n",
       "      <td>1.10E+08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>allenai/scibert_scivocab_cased</td>\n",
       "      <td>1.10E+08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>allenai/scibert_scivocab_uncased</td>\n",
       "      <td>1.10E+08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>priyankaranade/cybert</td>\n",
       "      <td>1.11E+08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>tram_multi_label_model</td>\n",
       "      <td>1.10E+08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>ehsanaghaei/SecureBERT</td>\n",
       "      <td>1.25E+08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>sentence-transformers/all-mpnet-base-v2</td>\n",
       "      <td>1.10E+08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>basel/ATTACK-BERT</td>\n",
       "      <td>1.10E+08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>qcri-cs/SentSecBert_10k</td>\n",
       "      <td>8.35E+07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Salesforce/SFR-Embedding-2_R</td>\n",
       "      <td>7.11E+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>BAAI/bge-en-icl</td>\n",
       "      <td>7.11E+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>nvidia/NV-Embed-V2</td>\n",
       "      <td>7.85E+09</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     models parameters\n",
       "0                         bert-base-uncased   1.10E+08\n",
       "1                           bert-base-cased   1.08E+08\n",
       "2                   FacebookAI/roberta-base   1.25E+08\n",
       "3               FacebookAI/xlm-roberta-base   2.78E+08\n",
       "4                  FacebookAI/roberta-large   3.55E+08\n",
       "5              FacebookAI/xlm-roberta-large   5.60E+08\n",
       "6                           s2w-ai/DarkBERT   1.25E+08\n",
       "7                         jackaduma/SecBERT   8.35E+07\n",
       "8                      jackaduma/SecRoBERTa   8.35E+07\n",
       "9                     markusbayer/CySecBERT   1.10E+08\n",
       "10           allenai/scibert_scivocab_cased   1.10E+08\n",
       "11         allenai/scibert_scivocab_uncased   1.10E+08\n",
       "12                    priyankaranade/cybert   1.11E+08\n",
       "13                   tram_multi_label_model   1.10E+08\n",
       "14                   ehsanaghaei/SecureBERT   1.25E+08\n",
       "15  sentence-transformers/all-mpnet-base-v2   1.10E+08\n",
       "16                        basel/ATTACK-BERT   1.10E+08\n",
       "17                  qcri-cs/SentSecBert_10k   8.35E+07\n",
       "18             Salesforce/SFR-Embedding-2_R   7.11E+09\n",
       "19                          BAAI/bge-en-icl   7.11E+09\n",
       "20                       nvidia/NV-Embed-V2   7.85E+09"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "pd.DataFrame(table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
