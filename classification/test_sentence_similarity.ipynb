{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test BERT-models for sentence similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import loader\n",
    "from const import MODEL_SENTENCE_SIM\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "import numpy as np\n",
    "from sklearn.metrics import f1_score\n",
    "import pandas as pd\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load configuration files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the precalculated embeddings.\n",
    "with open(\"datasets/mitre_embeddings.pickle\", \"rb\") as f:\n",
    "    store = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from const import TRAM_TECHNIQUES_10_LABELS, TRAM_TECHNIQUES_25_LABELS, TRAM_TECHNIQUES_LABELS, BOSCH_TECHNIQUES_LABELS, BOSCH_TECHNIQUES_10_LABELS, BOSCH_TECHNIQUES_25_LABELS, BOSCH_TECHNIQUES_50_LABELS\n",
    "\n",
    "\n",
    "def load_dataset(dataset_name, label_set):\n",
    "    # load dataset. this time, we get a pandas DataFrame, as we don't need\n",
    "    # to do any training. we just need to get the embeddings out of the\n",
    "    # sentences\n",
    "\n",
    "    LABEL_MAP = {\n",
    "        \"bosch_t\": BOSCH_TECHNIQUES_LABELS,\n",
    "        \"bosch_t10\": BOSCH_TECHNIQUES_10_LABELS,\n",
    "        \"bosch_t25\": BOSCH_TECHNIQUES_25_LABELS,\n",
    "        \"bosch_t50\": BOSCH_TECHNIQUES_50_LABELS,\n",
    "        \"tram\": TRAM_TECHNIQUES_LABELS,\n",
    "        \"tram_10\": TRAM_TECHNIQUES_10_LABELS,\n",
    "        \"tram_25\": TRAM_TECHNIQUES_25_LABELS,\n",
    "        \"bosch+tram\": sorted(list(set(BOSCH_TECHNIQUES_LABELS).union(set(TRAM_TECHNIQUES_LABELS)))),\n",
    "        \"all_mitre\": sorted(list(store.keys()))\n",
    "    }\n",
    "\n",
    "    if dataset_name == \"bosch\":\n",
    "        df = loader.load_datasets_for_tuning_embedding_threshold(\"bosch_t\")\n",
    "    elif dataset_name == \"tram\":\n",
    "        df = loader.load_datasets_for_tuning_embedding_threshold(\"tram\")\n",
    "    elif dataset_name == \"bosch+tram\":\n",
    "        df_bosch = loader.load_datasets_for_tuning_embedding_threshold(\"bosch_t\")\n",
    "        df_tram = loader.load_datasets_for_tuning_embedding_threshold(\"tram\")\n",
    "        df = pd.concat([df_bosch, df_tram])\n",
    "        df.reset_index(drop=True, inplace=True)\n",
    "    else:\n",
    "        raise Exception\n",
    "    \n",
    "    labels = LABEL_MAP[label_set]\n",
    "    return df, labels\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tune threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from const import MODEL_SENTENCE_SIM\n",
    "# load all models. all-mpnet-base-v2 is not listed because it is only\n",
    "# used in calculating the embeddings.\n",
    "# the other models are also used for fine-tuning on supervised tasks.\n",
    "# model_names_all = MODELS + [\"sentence-transformers/all-mpnet-base-v2\"]\n",
    "model_names_all = MODEL_SENTENCE_SIM\n",
    "# model_names = [\"bert-base-uncased\"] + [\"sentence-transformers/all-mpnet-base-v2\"]\n",
    "models = []\n",
    "model_names = []\n",
    "for mdl in model_names_all:\n",
    "    try:\n",
    "        models.append(loader.load_model_for_embedding(mdl).to(\"cuda:1\"))\n",
    "        model_names.append(mdl)\n",
    "    except:\n",
    "        print(\"Failed loading %s\" % mdl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_hyperparams(model, model_name, df, ttps):\n",
    "    store_loc = {k: v for k,v in store.items() if k in ttps}\n",
    "    output = {}\n",
    "    # they are already sorted, but who knows, sort them again\n",
    "    # as for some reason the multilabelbinarizer decides to sort them on\n",
    "    # its own. thus if you convert back to ttps and they do not follow\n",
    "    # the same order, all the labels are wrong :D\n",
    "    # ttps = sorted(list(store.keys()))\n",
    "    lb = MultiLabelBinarizer()\n",
    "    lb.fit([ttps])\n",
    "    print(len(ttps))\n",
    "\n",
    "    for index, row in tqdm(df.iterrows(), total=df.shape[0]):\n",
    "\n",
    "        output[index] = {}\n",
    "        output[index][\"labels\"] = list(set(row.labels).intersection(ttps))\n",
    "\n",
    "        sentence_emb = model.encode(row.sentence)\n",
    "        ttp_embs = np.array(\n",
    "            [store_loc[ttp][model_name][\"emb\"] for ttp in store_loc]\n",
    "        ).reshape(-1, sentence_emb.shape[0])\n",
    "        \n",
    "        sim = model.similarity([sentence_emb], ttp_embs)\n",
    "        output[index][\"sim\"] = sim\n",
    "\n",
    "    best_f1 = -1\n",
    "    best_tau = 0.5\n",
    "\n",
    "    print(\"Finding best wt, tau...\")\n",
    "    for tau in np.linspace(0.25, 0.75, num=20):\n",
    "        all_labels = []\n",
    "        all_preds = []\n",
    "\n",
    "        for k in list(output.keys()):\n",
    "            labels = output[k][\"labels\"]\n",
    "            ttp_sims = output[k][\"sim\"]\n",
    "            preds = torch.nonzero(ttp_sims[0] > tau).view(-1).tolist()\n",
    "            preds = [list(store_loc.keys())[index] for index in preds]\n",
    "            all_labels.append(labels)\n",
    "            all_preds.append(preds)\n",
    "\n",
    "        y_true = lb.transform(all_labels)\n",
    "        y_pred = lb.transform(all_preds)\n",
    "        f1 = f1_score(y_true, y_pred, average=\"macro\", zero_division=0.0)\n",
    "        if f1 > best_f1:\n",
    "            best_f1 = f1\n",
    "            best_tau = tau\n",
    "\n",
    "    return best_f1, best_tau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_params = {}\n",
    "\n",
    "\n",
    "for dataset_name in [\"tram\", \"bosch\"]:\n",
    "    model_params[dataset_name] = {}\n",
    "\n",
    "    for label_set in [\"tram\", \"tram_10\", \"tram_25\", \"bosch+tram\", \"all_mitre\", \"bosch_t\", \"bosch_t10\", \"bosch_t25\", \"bosch_t50\"]:\n",
    "\n",
    "        # if (dataset_name == \"bosch\" and \"tram\" in label_set) or (dataset_name == \"tram\" and \"bosch\" in label_set) and dataset_name != \"bosch+tram\":\n",
    "        #     continue\n",
    "\n",
    "        df, labels = load_dataset(dataset_name, label_set)\n",
    "        model_params[dataset_name][label_set] = {}\n",
    "\n",
    "        # for each row in the dataset:\n",
    "        # 1) calculate the embeddings for each model\n",
    "        # 2) calculate the similarity to each encoded TTP title and TTP description\n",
    "        #    making sure to consider the one calculated by the corresponding model.\n",
    "        #    This can be easily done, as we pre-generated the vectors and saved them\n",
    "        #    using the model name as the key\n",
    "        # for model, model_name in zip([models[-1]], [\"sentence-transformers/all-mpnet-base-v2\"]):\n",
    "        for model, model_name in zip(models, model_names):\n",
    "            print(f\"> {model_name}, {dataset_name}, {label_set}\")\n",
    "\n",
    "            best_f1, best_tau = find_hyperparams(model, model_name, df, labels)\n",
    "            model_params[dataset_name][label_set][model_name] = {\n",
    "                \"tau\": best_tau\n",
    "            }\n",
    "\n",
    "            print(\n",
    "                f\"best_f1={best_f1}, best_tau={best_tau}\\n\"\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open(\"configs/tram_sentence_similarity.json\", \"w\") as f:\n",
    "    json.dump(model_params, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_file = \"configs/tram_sentence_similarity.json\"\n",
    "\n",
    "# load json config file\n",
    "with open(config_file, \"r\") as f:\n",
    "    config = json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "uncomment for \"experiment on open classification\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# config = {\n",
    "#     \"bosch\": {\n",
    "#         \"bosch_t\": {\n",
    "#             \"sentence-transformers/all-mpnet-base-v2\": {\n",
    "#                 \"tau\": 0.5\n",
    "#             },\n",
    "#             \"basel/ATTACK-BERT\": {\n",
    "#                 \"tau\": 0.5\n",
    "#             },\n",
    "#             \"qcri-cs/SentSecBert_10k\": {\n",
    "#                 \"tau\": 0.5\n",
    "#             }\n",
    "#         },\n",
    "#         \"bosch_t10\": {\n",
    "#             \"sentence-transformers/all-mpnet-base-v2\": {\n",
    "#                 \"tau\": 0.5\n",
    "#             },\n",
    "#             \"basel/ATTACK-BERT\": {\n",
    "#                 \"tau\": 0.5\n",
    "#             },\n",
    "#             \"qcri-cs/SentSecBert_10k\": {\n",
    "#                 \"tau\": 0.5\n",
    "#             }\n",
    "#         },\n",
    "#         \"bosch_t25\": {\n",
    "#             \"sentence-transformers/all-mpnet-base-v2\": {\n",
    "#                 \"tau\": 0.5\n",
    "#             },\n",
    "#             \"basel/ATTACK-BERT\": {\n",
    "#                 \"tau\": 0.5\n",
    "#             },\n",
    "#             \"qcri-cs/SentSecBert_10k\": {\n",
    "#                 \"tau\": 0.5\n",
    "#             }\n",
    "#         },\n",
    "#         \"bosch_t50\": {\n",
    "#             \"sentence-transformers/all-mpnet-base-v2\": {\n",
    "#                 \"tau\": 0.5\n",
    "#             },\n",
    "#             \"basel/ATTACK-BERT\": {\n",
    "#                 \"tau\": 0.5\n",
    "#             },\n",
    "#             \"qcri-cs/SentSecBert_10k\": {\n",
    "#                 \"tau\": 0.5\n",
    "#             }\n",
    "#         },\n",
    "#         \"all_mitre\": {\n",
    "#             \"sentence-transformers/all-mpnet-base-v2\": {\n",
    "#                 \"tau\": 0.5\n",
    "#             },\n",
    "#             \"basel/ATTACK-BERT\": {\n",
    "#                 \"tau\": 0.5\n",
    "#             },\n",
    "#             \"qcri-cs/SentSecBert_10k\": {\n",
    "#                 \"tau\": 0.5\n",
    "#             }\n",
    "#         }\n",
    "#     }\n",
    "# }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model_per_document(model, model_name, df, dataset_name, ttps, tau):\n",
    "    store_loc = {k: v for k,v in store.items() if k in ttps}\n",
    "\n",
    "    if \"bosch\" in dataset_name:\n",
    "        col = \"document\"\n",
    "    elif \"tram\" in dataset_name:\n",
    "        col = \"doc_title\"\n",
    "    else:\n",
    "        raise AttributeError\n",
    "\n",
    "    out = {}\n",
    "    for doc_name, df_doc in tqdm(df.groupby(col)):\n",
    "        all_labels = []\n",
    "        all_preds = []\n",
    "\n",
    "        for _, row in df_doc.iterrows():\n",
    "\n",
    "            labels = list(set(row.labels).intersection(ttps))\n",
    "            \n",
    "            sentence_emb = model.encode(row.sentence)\n",
    "            ttp_embs = np.array(\n",
    "                [store_loc[ttp][model_name][\"emb\"] for ttp in store_loc]\n",
    "            ).reshape(-1, sentence_emb.shape[0])\n",
    "            \n",
    "            ttp_sim = model.similarity(sentence_emb, ttp_embs)\n",
    "\n",
    "            preds = torch.nonzero(ttp_sim[0] > tau).view(-1).tolist()\n",
    "            preds = [list(store_loc.keys())[index] for index in preds]\n",
    "            all_labels.extend(labels)\n",
    "            all_preds.extend(preds)\n",
    "\n",
    "        out[doc_name] = {\n",
    "            \"labels\": list(set(all_labels)),\n",
    "            \"preds\": list(set(all_preds))\n",
    "        }\n",
    "\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from test_common import calc_results_per_document\n",
    "from const import *\n",
    "\n",
    "def load_dataset_test(dataset_name, label_set):\n",
    "    # load dataset. this time, we get a pandas DataFrame, as we don't need\n",
    "    # to do any training. we just need to get the embeddings out of the\n",
    "    # sentences\n",
    "\n",
    "    LABEL_MAP = {\n",
    "        \"bosch_t\": BOSCH_TECHNIQUES_LABELS,\n",
    "        \"bosch_t10\": BOSCH_TECHNIQUES_10_LABELS,\n",
    "        \"bosch_t25\": BOSCH_TECHNIQUES_25_LABELS,\n",
    "        \"bosch_t50\": BOSCH_TECHNIQUES_50_LABELS,\n",
    "        \"tram\": TRAM_TECHNIQUES_LABELS,\n",
    "        \"tram_10\": TRAM_TECHNIQUES_10_LABELS,\n",
    "        \"tram_25\": TRAM_TECHNIQUES_25_LABELS,\n",
    "        \"bosch+tram\": sorted(list(set(BOSCH_TECHNIQUES_LABELS).union(set(TRAM_TECHNIQUES_LABELS)))),\n",
    "        \"all_mitre\": sorted(list(store.keys()))\n",
    "    }\n",
    "\n",
    "    if dataset_name == \"bosch\":\n",
    "        df_test = loader.load_datasets_for_testing_embedding_threshold(\"bosch_t\")\n",
    "    elif dataset_name == \"tram\":\n",
    "        df_test = loader.load_datasets_for_testing_embedding_threshold(\"tram\")\n",
    "    elif dataset_name == \"bosch+tram\":\n",
    "        df_bosch = loader.load_datasets_for_testing_embedding_threshold(\"bosch_t\")\n",
    "        df_tram = loader.load_datasets_for_testing_embedding_threshold(\"tram\")\n",
    "        df_test = pd.concat([df_bosch, df_tram])\n",
    "        df_test.reset_index(drop=True, inplace=True)\n",
    "    else:\n",
    "        raise Exception\n",
    "    \n",
    "    labels = LABEL_MAP[label_set]\n",
    "    return df_test, labels\n",
    "\n",
    "output = {\n",
    "    \"model_name\": [],\n",
    "    \"dataset_name\": [],\n",
    "    \"label_set\": [],\n",
    "    \"f1\": [],\n",
    "    \"accuracy\": [],\n",
    "    \"precision\": [],\n",
    "    \"recall\": []\n",
    "}\n",
    "\n",
    "for dataset in config:\n",
    "    for label_set in config[dataset]:\n",
    "        df_test, labels = load_dataset_test(dataset, label_set)\n",
    "        for model_name in config[dataset][label_set]:\n",
    "            model = loader.load_model_for_embedding(model_name)\n",
    "            print(f\"> testing {model_name}\")\n",
    "            tau = config[dataset][label_set][model_name][\"tau\"]\n",
    "            model = loader.load_model_for_embedding(model_name)\n",
    "            out_df = test_model_per_document(model, model_name, df_test, dataset, labels, tau)\n",
    "            results_df = calc_results_per_document(out_df)\n",
    "            f1 = results_df.f1.mean()\n",
    "            accuracy = results_df.accuracy.mean()\n",
    "            precision = results_df.precision.mean()\n",
    "            recall = results_df.recall.mean()\n",
    "            print(f1)\n",
    "            output[\"model_name\"].append(model_name)\n",
    "            output[\"dataset_name\"].append(dataset)\n",
    "            output[\"label_set\"].append(label_set)\n",
    "            output[\"f1\"].append(f1)\n",
    "            output[\"accuracy\"].append(accuracy)\n",
    "            output[\"precision\"].append(precision)\n",
    "            output[\"recall\"].append(recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(output).sort_values(by=\"f1\").to_csv(\"test_results/output.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
